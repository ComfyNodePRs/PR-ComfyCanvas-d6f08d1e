{
  "4": {
    "inputs": {
      "ckpt_name": "dreamshaper_8.safetensors"
    },
    "class_type": "CheckpointLoaderSimple",
    "_meta": {
      "title": "Load Checkpoint"
    }
  },
  "6": {
    "inputs": {
      "text": [
        "58",
        2
      ],
      "clip": [
        "4",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "7": {
    "inputs": {
      "text": "text, watermark",
      "clip": [
        "4",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "8": {
    "inputs": {
      "samples": [
        "34",
        0
      ],
      "vae": [
        "4",
        2
      ]
    },
    "class_type": "VAEDecode",
    "_meta": {
      "title": "VAE Decode"
    }
  },
  "15": {
    "inputs": {
      "weight": 0.5,
      "weight_type": "linear",
      "combine_embeds": "concat",
      "start_at": 0,
      "end_at": 1,
      "embeds_scaling": "K+V",
      "model": [
        "16",
        0
      ],
      "ipadapter": [
        "16",
        1
      ],
      "image": [
        "48",
        2
      ]
    },
    "class_type": "IPAdapterAdvanced",
    "_meta": {
      "title": "IPAdapter Advanced"
    }
  },
  "16": {
    "inputs": {
      "preset": "STANDARD (medium strength)",
      "model": [
        "4",
        0
      ]
    },
    "class_type": "IPAdapterUnifiedLoader",
    "_meta": {
      "title": "IPAdapter Unified Loader"
    }
  },
  "17": {
    "inputs": {
      "lora_name": "pytorch_lora_weights_lcm.safetensors",
      "strength_model": 1,
      "strength_clip": 1,
      "model": [
        "15",
        0
      ],
      "clip": [
        "4",
        1
      ]
    },
    "class_type": "LoraLoader",
    "_meta": {
      "title": "Load LoRA"
    }
  },
  "31": {
    "inputs": {
      "strength": 1,
      "conditioning": [
        "6",
        0
      ],
      "control_net": [
        "32",
        0
      ],
      "image": [
        "46",
        0
      ]
    },
    "class_type": "ControlNetApply",
    "_meta": {
      "title": "Apply ControlNet"
    }
  },
  "32": {
    "inputs": {
      "control_net_name": "control_lora_rank128_v11p_sd15_inpaint_fp16.safetensors"
    },
    "class_type": "ControlNetLoader",
    "_meta": {
      "title": "Load ControlNet Model"
    }
  },
  "34": {
    "inputs": {
      "seed": 53176650469610,
      "steps": 4,
      "cfg": 1.5,
      "sampler_name": "lcm",
      "scheduler": "sgm_uniform",
      "denoise": 0.5,
      "model": [
        "35",
        0
      ],
      "positive": [
        "31",
        0
      ],
      "negative": [
        "7",
        0
      ],
      "latent_image": [
        "55",
        0
      ]
    },
    "class_type": "KSampler",
    "_meta": {
      "title": "KSampler"
    }
  },
  "35": {
    "inputs": {
      "sampling": "lcm",
      "zsnr": false,
      "model": [
        "17",
        0
      ]
    },
    "class_type": "ModelSamplingDiscrete",
    "_meta": {
      "title": "ModelSamplingDiscrete"
    }
  },
  "46": {
    "inputs": {
      "image": [
        "48",
        0
      ],
      "mask": [
        "49",
        0
      ]
    },
    "class_type": "InpaintPreprocessor",
    "_meta": {
      "title": "InpaintPreprocessor"
    }
  },
  "48": {
    "inputs": {
      "seed": 934164250824628
    },
    "class_type": "OutpaintCanvasTool",
    "_meta": {
      "title": "OutpaintCanvasTool"
    }
  },
  "49": {
    "inputs": {
      "channel": "red",
      "image": [
        "48",
        1
      ]
    },
    "class_type": "ImageToMask",
    "_meta": {
      "title": "Convert Image to Mask"
    }
  },
  "50": {
    "inputs": {
      "image": [
        "8",
        0
      ]
    },
    "class_type": "stitch",
    "_meta": {
      "title": "stitch"
    }
  },
  "51": {
    "inputs": {
      "filename_prefix": "ComfyUI",
      "images": [
        "50",
        0
      ]
    },
    "class_type": "SaveImage_Canvas",
    "_meta": {
      "title": "SaveImage_Canvas"
    }
  },
  "52": {
    "inputs": {
      "images": [
        "48",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Preview Image"
    }
  },
  "53": {
    "inputs": {
      "images": [
        "48",
        1
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Preview Image"
    }
  },
  "54": {
    "inputs": {
      "images": [
        "48",
        2
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Preview Image"
    }
  },
  "55": {
    "inputs": {
      "samples": [
        "56",
        0
      ],
      "mask": [
        "49",
        0
      ]
    },
    "class_type": "SetLatentNoiseMask",
    "_meta": {
      "title": "Set Latent Noise Mask"
    }
  },
  "56": {
    "inputs": {
      "pixels": [
        "48",
        0
      ],
      "vae": [
        "4",
        2
      ]
    },
    "class_type": "VAEEncode",
    "_meta": {
      "title": "VAE Encode"
    }
  },
  "57": {
    "inputs": {
      "model": "microsoft/Florence-2-base",
      "precision": "fp16",
      "attention": "eager"
    },
    "class_type": "DownloadAndLoadFlorence2Model",
    "_meta": {
      "title": "DownloadAndLoadFlorence2Model"
    }
  },
  "58": {
    "inputs": {
      "text_input": "",
      "task": "more_detailed_caption",
      "fill_mask": true,
      "keep_model_loaded": false,
      "max_new_tokens": 1024,
      "num_beams": 3,
      "do_sample": true,
      "image": [
        "48",
        0
      ],
      "florence2_model": [
        "57",
        0
      ]
    },
    "class_type": "Florence2Run",
    "_meta": {
      "title": "Florence2Run"
    }
  }
}